---
parent: Events
title: Redefining Domain Adaptation for Machine Translation and Voice Recognition - An Essential Primer
description: An updated perspective on domain adatpation for MT and ASR utilizing modern techniques
location: online
name: Redefining Domain Adaptation for Machine Translation and Voice Recognition - An Essential Primer
startDate: 2023-01-26

seo:
  type: Event
  name: Redefining Domain Adaptation for Machine Translation and Voice Recognition - An Essential Primer
  startDate: 2023-02-16
  endDate: 2023-02-16
  eventAttendanceMode: OnlineEventAttendanceMode
  eventStatus: EventScheduled

  location:
    type: VirtualLocation
    url: https://www.omniscien.com/resources/webinars/

  organizer:
    type: Organization
    name: Omniscien Technologies
    url: https://www.omniscien.com
---


Large Language Models (such as ChatGPT, Machine Translation (MT) and Automated Speech Recognition (ASR) are three of the many different Artificial Intelligence (AI) technologies today that use Transformer models, so their domain adaptation (aka customization) processes have a lot in common. Domain Adaptation (also known as customization) is the adaptation of AI models used in MT and ASR to a specific domain or purpose such as automotive legal documentation, medical discussions, or investment portfolio sales. 

This webinar will demonstrate how Omniscien’s team gathers and synthesizes billions of sentences of training data that is used to teach and adapt AI systems. We will explore best practices and explain the limitations of traditional approaches. We will show how to use AI to create many millions of sentences of training data that can be used as part of domain adaptation. Essentially, this means using one form of AI to teach another AI form new knowledge.

Modern AI systems are built on foundation models containing billions of parameters and need modern approaches to creating training data. New approaches are needed. The legacy historical approaches of simply uploading a few thousand sentences of data are insufficient. Some simple examples are listed below:
- 10K-500K sentences that an organization can typically provide are insufficient to have any notable impact against billions of parameters used in modern foundation models.
- Translation Management Systems (TMS) such as memoQ, Trados Studio, XTM, etc., do not store translation memories in optimal formats for training modern AI algorithms. Translation memories are not in sequence of the original document and are de-duplicated making them unsuitable for training document level translation systems where entire bilingual documents in sequence are needed to obtain the correct context.
- Language Service Providers (LSPs) and translation departments may keep translation memories classified by organisation or customer, but seldom by writing style or purpose. Content from sales, marketing, engineering, legal and other areas are all mixed together.
- Glossaries alone are absolute and result in over-fitting in the wrong context. Many MT systems using glossaries simply search and replace rather than use the glossary in context and the use of the glossary is not 100% reliable.

We will also explore how the advantages that LSPs and large organizations have had in the past by having “larger amounts of training data” on file are very quickly becoming irrelevant in the new redefined approach to domain adaptation where tens or evens hundreds of millions of sentences are needed to have any meaningful effect. Fortunately, there are already solutions that address this issue by enabling any organization to create this data quickly and cost effectively. 
### Speakers

- Professor Philipp Koehn, Chief Scientist at Omniscien + Professor at Johns Hopkins University
- Dion Wiggins, Chief Technology Officer at Omniscien

### Links

- [Event page](https://www.omniscien.com/resources/webinars/)
- [Recording](https://www.omniscien.com/resources/webinars/)
